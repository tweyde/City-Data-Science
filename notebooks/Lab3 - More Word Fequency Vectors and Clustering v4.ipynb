{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "name": "python", 
            "version": "3.6.3", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python with Pixiedust (Spark 2.2)", 
            "name": "pythonwithpixiedustspark22"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "# Lab Sheet 3: Extracting Word Fequency Vectors with Spark\n\nThese tasks are for working in the lab session and during the week. We will use the same data as last week (19 files in './City-Data-Science/library/') and use some more RDD functions. We will apply two different approaches to create and use fixed size vectors.\n\nFirst update the repo."
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "!git clone https://github.com/tweyde/City-Data-Science.git\n%cd City-Data-Science/\n!git pull https://github.com/tweyde/City-Data-Science.git\n%cd ..", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Here is code from last week that we run first, and then extend. "
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "import re \n\ndef stripFinalS( word ):\n    word = word.lower() # lower case\n    if len(word) >0 and word[-1] == 's': # check for final letter\n        return word[:-1]\n    else:\n        return word\n    \ndef splitFileWords(filenameContent): # your splitting function\n    f,c = filenameContent # split the input tuple  \n    fwLst = [] # the new list for (filename,word) tuples\n    wLst = re.split('\\W+',c) # <<< now create a word list wLst\n    for w in wLst : # iterate through the list\n        fwLst.append((f,stripFinalS(w))) # and append (f,w) to the \n    return fwLst #return a list of (f,w) tuples \n\nfrom pyspark import SparkContext\n\nsc = SparkContext.getOrCreate()\n\ndirPath = './City-Data-Science/library/' #  path\nft_RDD = sc.wholeTextFiles(dirPath) # create an RDD with wholeTextFiles\nfnt_RDD = ft_RDD.map(lambda ft: (re.split('[/\\.]',ft[0])[-2],ft[1])) # just take filenames, \n                                                # drop path and extension for readability\nfw_RDD1 = fnt_RDD.flatMap(splitFileWords) # split words per file, strip final 's'\nfw_RDD = fw_RDD1.filter(lambda fw: fw[1] not in ['','project','gutenberg', 'ebook']) # get rid of some unwanted words\nfw_RDD.take(3)\n# output should look like this: [('emma', 'the'), ('emma', 'of'), ('emma', 'emma')]", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 1) Warm-up\nLet's start with a few small tasks, to become more fluent with RDDs and lambda expressions.\n\na) Count the number of documents.\nb) Determine the number distinct words in total (the vocabulary size) using RDD.distinct(). This involves removing the fs from the (f,w) pairs and geting getting the RDD size (with RDD.count()). \nc) Get the number of words (including repeated ones) per book. \nd) Determine the number of distinct words per book. This involves determining the disting (f,w) pairs, geting a list of words per file, and getting the list size.\ne) Count the average number of occurences per word per file (words/vocabulary). Use RDD.join() to get both numbers into one RDD. \n\nRemember that `>>>` indicates a line where you should do something - you need to remove it for any code to work. \nTypically, you'll find a `...` placeholder in that line at the place where you should add the code.  "
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# a) Library size\n>>>print(\"Number of documents: \",ft_RDD ...) # count the number of docs\n\n# b) Vocabulary size\n>>>w_RDD = fw_RDD.map( ... ) # remove the file names, keep just the words\n>>>w_RDDu = w_RDD # keep only one unique instance of every word\nprint('Total vocabulary size: ',w_RDDu.count()) # \n\n\n# c) words per book\nfrom operator import add\n>>>f1_RDD = fw_RDD.map(lambda fw: ...) # swap and wrap (f,w) to (f,1)\n>>>fc_RDD = f1_RDD.reduceByKey(... ) # add the 1s up\nprint('Words per book: ',fc_RDD.take(3))\n>>> extra task: try also to express this with one function that appeared in the lecture today\n\n# d) Vocabulary per book\nfw_RDDu = fw_RDD.distinct() # get unique (f,w) pairs - i.e. evey word only once per file. I use postfix u to mark 'unique'\nf1_RDDu = fw_RDDu.map(lambda fw: (fw[0],1)) # wrap (f,w) to (f,1)\nfcu_RDD = f1_RDDu.reduceByKey(add) # add the 1s up\nprint('Vocabulary per book: ',fcu_RDD.take(3)) \n>>> extra task: try also replacing the map and reduce by one function \n\n# e) Average occurences of words per book (i.e. words/vocab per book)\n>>>f_wv_RDD = fc_RDD ... fcu_RDD # join the two RDDs to get (f,(w,v)) tuples\nprint(f_wv_RDD.take(3)) \n>>>f_awo_RDD = f_wv_RDD.map(lambda f_wv: (f_wv[0], ... )) # this is the tricky part. \n            # Resolve nested tuples in the lambda to get (filename,words/vocab) tuples\nprint('Average word occurences: ',f_awo_RDD.take(3))\n# should look like this [('henry_V', 6.237027812370278), ('king_lear', 7.815661103979461), ('lady_susan', 8.531763947113834)]", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "\n## 2) Fixed vectors: Reduced vocabulary approach\n\nThe first task in this lab is to use a reduced vocabulary, only the stopwords from a list, to make sure that we have a fixed size vector. This is a common approach in stylometry. The problem is that some stopwords might not appear in some documents. We will deal with that by creating an RDD with ((f,w),0) tuples that we then merge with the ((f,w),count) RDD. \n\nStart by running the code above, then you can add 1s and use reduceByKey(add) like last week to get the counts of the words per filename.\n\nThen, please make sure that all stopwords are present by creating a new RDD that contains the keys of the fw_RDD, i.e. the filenames, using the keys() method of class RDD. Then you can use flatMap to create a [((filname,stopword),0), ...] list, using a list comprehension. The 0s should not be 1s, as we don't want add to add extra counts.\nThe RDD with ((filename,stopword),0) tuples can then be merged with fw_RDD2 using union(). Then you can count as normal."
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "from operator import add\n\nstopwlst = ['the','a','in','of','on','at','for','by','i','you','me'] # stopword list\n>>>fw_RDD2 = fw_RDD.filter(lambda x: ... ) # filter, keeping only stopwords\n\n>>>fsw_0_RDD = fw_RDD.keys().flatMap(lambda f: [((f,sw),0) for sw in stopwlst])\nprint(fsw_0_RDD.take(3))\n\n>>>fw_1_RDD = fw_RDD2.map(lambda fw: ...)  #<<< change (f,w) to ((f,w),1)\nprint(fw_1_RDD.take(3))\n\n>>>fw_10_RDD = fw_1_RDD ... fsw_0_RDD #<<< create the union on the two RDDs\nprint(fw_10_RDD.take(3))\n\nfw_c_RDD = fw_10_RDD.reduceByKey(add) #<<< count the words\nprint(fw_c_RDD.take(3))\n# output should look like this:\n#[(('emma', 'the'), 0), (('emma', 'a'), 0), (('emma', 'in'), 0)]\n#[(('emma', 'the'), 1), (('emma', 'of'), 1), (('emma', 'by'), 1)]\n#[(('emma', 'the'), 1), (('emma', 'of'), 1), (('emma', 'by'), 1)]\n#[(('emma', 'the'), 5380), (('emma', 'by'), 591), (('emma', 'you'), 2068)]", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 3) Creating sorted lists\n\nAs a next step, map the `((filename,word),count)` to `( filename, [ (word, count) ])` using the function `reGrpLst` to regroup and create a list. \n\nThen sort the [(word,count),...] lists in the values (i.e. 2nd part of the tuple) with the the words as keys. Have a [look at the Python docs](https://docs.python.org/3.5/library/functions.html?highlight=sorted#sorted) for how to do this. Hint: use a lambda that extracts the words as the key, e.g. `sorted(f_wdL[1], key = lambda wc: ... )`.   "
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "def reGrpLst(fw_c): # we get a nested tuple\n    >>>     # split the outer tuple\n    >>>     # split the inner tuple\n    return (f,[(w,c)]) # return (f,[(w,c)]) structure. Can be used verbatim, if your variable names match.\n\n\n>>>f_wcL_RDD = fw_c_RDD.map(reGrpLst) # apply reGrpLst\nf_wcL2_RDD = f_wcL_RDD.reduceByKey(add) # create [(w,c), ... ,(w,c)] lists per file \n>>>f_wcLsort_RDD = f_wcL2_RDD.map(lambda f_wcL: (f_wcL[0], sorted(...))) #<<< sort the word count lists by word\nprint(f_wcLsort_RDD.take(3)) \n>>>f_wVec_RDD = f_wcLsort_RDD.map(lambda f_wc: (f_wc[0],[float(c) for ...])) # remove the words from the wc pairs and convert the numbers to floats\nf_wVec_RDD.take(3)\n# output:\n# [('lady_susan', [('a', 611), ('at', 161), ('by', 152), ('for', 262), ('i', 1106), ('in', 402), ('me', 200), ('of', 787), ...", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 4) Clustering\n\nNow we have feature vectors of fixed size, we can use KMeans as provided by Spark.\n\nThe files in our library are by two authors. After clustering, check if the cluters reflect authorship:\n\nWILLIAM SHAKESPEARE: \nmerchant_of_venice, \nrichard_III, \nmidsummer,\ntempest,\nromeoandjuliet,\nothello,\nhenry_V,\nmacbeth,\nking_lear,\njulius_cesar,\nhamlet\n\nJANE AUSTEN\nmansfield_park,\nemma,\nnorthanger_abbey,\nlady_susan,\npersuasion,\nprideandpredjudice,\nsenseandsensibility"
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "\nfrom math import sqrt\n\nfrom pyspark.mllib.clustering import KMeans #, KMeansModel\n\n#print('f_wVec_RDD.take(2): ', f_wVec_RDD.take(1))\n>>>wVec_RDD = f_wVec_RDD.map(lambda f_wcl: ...) # strip the filenames, keep only the vectors\n#print(wVec_RDD.take(3))\n\n# Build the model (cluster the data)\nclusterModel = KMeans.train(wVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n\n# Assign the filenames to the clusters\nfc_RDD = f_wVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1])))\nfor s in fc_RDD.collect():\n    print(s)\n\n# Evaluate clustering by computing Within Set Sum of Squared Errors\ndef error(point):\n    center = clusterModel.centers[clusterModel.predict(point)]\n    return sqrt(sum([x**2 for x in (point - center)]))\n\nWSSSE = wVec_RDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)\nprint(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n# now check if the clusters match the authors\n# output:\n#('lady_susan', 1)\n#('macbeth', 1)\n#('merchant_of_venice', 1)\n#('othello', 1)\n#('persuasion', 0)\n#('emma', 0)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 5) Alternative approach: feature hashing\n\nInstead of the previous appraoch, we now use feature hashing, as done last week."
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "def hashing_vectorizer(word_count_list, N):\n    v = [0] * N  # create fixed size vector of 0s\n    for word_count in word_count_list: \n>>>        ...     # unpack tuple\n        h = hash(word)              # get hash value\n        v[h % N] = v[h % N] + count # add count\n    return v # return hashed word vector\n\nfrom operator import add\n\nN = 10\n\n# we use fw_RDD from the beginning with all the words, not just stopwords\n>>>fw_1_RDD = fw_RDD.map(lambda fw: ...)  #<<< change (f,w) to ((f,w),1)\nfw_c_RDD = fw_1_RDD.reduceByKey(add) #as above\nf_wcL_RDD = fw_c_RDD.map(reGrpLst) #as above\nf_wcL2_RDD = f_wcL_RDD.reduceByKey(add) #create [(w,c), ... ,(w,c)] lists per file \n>>>f_wVec_RDD = f_wcL2_RDD.map(lambda f_wcl: (f_wcl[0], ...)) # apply the hashing_vectorizer to the word-count list\nprint(f_wVec_RDD.take(3))\n# output:\n# [('henry_V', [2277, 2293, 1182, 1792, 2058, 1550, 787, 1821, 814, 1916, 902, 752, 1249, 1022, 888, 1702, 1357, 2886, 1007, 1645]), ('king_lear', [2149, 2217, 1010, 2167, 2331, 1372, 726, 1682, 747, 1623, 1470, 889, 1248, 1371, 1062, 1472, 1510, 2456, 1364, 1253]), ('lady_susan', [2015, 1850, 823, 1828, 2099, 1658, 704, 1656, 588, 1319, 1433, 789, 1051, 909, 748, 1236, 1290, 2195, 570, 1348])]", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "from math import sqrt\n\nfrom pyspark.mllib.clustering import KMeans #, KMeansModel\n\n#print('f_wVec_RDD.take(2): ', f_wVec_RDD.take(1))\nwVec_RDD = f_wVec_RDD.map(lambda f_wcl: f_wcl[1]) # strip the filenames\n#print(wVec_RDD.collect())\n\n# Build the model (cluster the data)\nclusterModel = KMeans.train(wVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n\n# Assign the files to the clusters\nfc_RDD = f_wVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1]))) \nfor s in fc_RDD.collect():\n    print(s)\n    \n# resusing 'error' function from abov\nWSSSE = wVec_RDD.map(lambda point: error(point)).reduce(lambda x, y: x + y)\nprint(\"Within Set Sum of Squared Error = \" + str(WSSSE))", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "## 6) Neutralising document length: normalised vectors\n\n'Lady Susan' ends up reliably in the wrong cluster. A possible explanation is that it is shorter than the other Austen works. Try normalising the word counts, i.e. by dividing by their sum. That takes away the effect of length. What is the effect on the clustering?\n    \nYou can use a list comprehension for the normalisation."
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": ">>>nwVec_RDD = wVec_RDD.map(lambda v: (...)) # provide a list comprehension that \n                            #normalises the values by dividing by the sum over the list\nprint(\"Normalised vectors: \",nwVec_RDD.take(3))\n\n# Build the model (cluster the data)\nclusterModel = KMeans.train(nwVec_RDD, 2, maxIterations=10, initializationMode=\"random\")\n\n# Assign the files to the clusters\nfc_RDD = f_wVec_RDD.map(lambda fv: (fv[0],clusterModel.predict(fv[1])))\nfor s in fc_RDD.collect():\n    print(s)\n# output\n# Normalised vectors:  [[0.07615384615384616, 0.07668896321070234, 0.03953177257525083, 0.05993311036789298, \n# ...                       \n# ('henry_V', 0)\n# ('king_lear', 0)\n#('lady_susan', 1)\n# ..", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 7) Building an index\n\nStarting from the fw_RDD we now start building the index and calculating the IDF values. Since we have the TF values alread, we only need to keep the unique filenames per word using [RDD.distinct()](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.distinct).  \nThen we create a list of filenames. The length of the list is the document frequency DF per word.\nFrom the DF value we can calculate the IDF value as log(18/DF) "
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "from math import log\n\nfwu_RDD = fw_RDD.distinct() # get unique file/word pairs\n>>>wfl_RDD = fwu_RDD.map(lambda fw: (fw[1],...)) # create (w,[f]) tuples \nwfL_RDD = wfl_RDD.reduceByKey(add) # concatenate the lists with 'add'\nprint(wfL_RDD.take(3))\n\n>>>wdf_RDD = wfL_RDD.map(lambda wfl: (wfl[0],...)) # get the DF replacing the file list with its lenght\nprint(\"DF: \",wdf_RDD.take(3))\n>>>widf_RDD = wdf_RDD.map(lambda wdf: (wdf[0],...))) # get he IDF by replacing DF with log(18/DF)\nprint(\"IDF: \",widf_RDD.take(3))\n# ouptut:\n# [('of', ['henry_V', 'king_lear', 'lady_susan', 'macbeth', 'merchant_of_venice', 'midsummer', 'northanger_abbey', \n# DF:  [('of', 18), ('shakespeare', 15), ('henry', 9)]\n# IDF:  [('of', 0.0), ('shakespeare', 0.1823215567939546), ('henry', 0.6931471805599453)]", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 1
}