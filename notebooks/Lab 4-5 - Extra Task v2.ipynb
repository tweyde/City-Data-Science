{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Lab Sheet 4-5: Extended Extra Task - full 20 Newsgroups\n\nHere is a detailled and extended version of extra task, multi-class classification on the full 20 newgroups.\n\n\nThere are a few new things here: \n- We need to use Multi-class classifiers and evaluators, see below for details.\n- I've organised the labelling differently, using a dictionary. \n- We need to specift the types of the columns, because the lables need to be numeric.\n- We need to reduce the size of the data and the running time of the training to make sure first that everything works. Once we know we can then switch to the full version. Look for the lines starting with `# REDUCED` and `# FULL` to switch from the reduced version to the full one. \n- I've  included saving RDDs and DataFrames to files and reading them back (in pickle and parquet format). Reading from small files and transforming can take a long time, so that saving the results is useful to avoid having to redo the processing.\n- I've also added a good number of `%time` commands to measure execution time. Note the differences between CPU Time and Wall Time.\n\nI've added the customary `>>>` gaps for you to fill in.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Preparation and troubleshooting\nLike in the main lab, we need to get the data. If you haven't got the data yet, uncomment the four cells below and run them once.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#%cd ~/notebook/work/City-Data-Science/datasets/\n#!wget http://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/20_newsgroups.tar.gz\n#!ls", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "#!tar -xf 20_newsgroups.tar.gz\n# '!' calls a program on the machine (the DSX service runs on virtual Linux machines).\n#print(\"+++ Unzipping finished. +++\")", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "#!echo \"datasets/20_newsgroups/**\" >> ~/notebook/work/City-Data-Science/.git/info/exclude\n# add the newly created directory to the list of excluded dirs to prevent accidental uploading to github\n# do this only once", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "# go into the new directory\n#%cd ./20_newsgroups/ \n#%ls # and show its content", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "The cell below is for deleting the metastore_db, if a related error occurs. You need to fill in the metastore_db path from the error message.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#!rm -R <...metastore_db>", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "# 1) Loading the data\nThe loading process is a bit different, as we now want to get all classes, not just 2. For this we will create a dictionary of topic names to class labels (as numbers), that we can use for the classifier.\n\nA new part is that we add labels using the ditionary created above. See here for how to create and use dictionaries: [https://docs.python.org/3/library/stdtypes.html#typesmapping](https://docs.python.org/3/library/stdtypes.html#typesmapping).\n\nTry getting everything to work with the REDUCED version first, then switch to the FULL version.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.ml import *\nfrom pyspark.sql import *\n\nfrom pyspark.sql.types import Row\nfrom pyspark.sql import SQLContext\n\nsc = SparkContext.getOrCreate()\nsqlContext = SQLContext(sc)\nspark = SparkSession.builder.getOrCreate()\n\nimport os.path\n\n%cd ~\np = os.path.abspath('./notebook/work/City-Data-Science/datasets/20_newsgroups/')\n# we need the absolute path, as the executors that will read the from directories, will not run in the same environment, so that %cd calls don't help\nprint(\"p: \", p) \n\n# We create a dictionary for class labels (from the directory names)\ntopicLabels = {}\nimport os\nlabel = 0\n# we iterate through the data directories \nfor root, dirs, files in os.walk(p, topdown=False):\n    for dirname in dirs: # dirs contains all directories in p\n        topicLabels[dirname] = label # assign a label number  \n        label = label + 1 # and increment the label number\n\nprint(topicLabels)\n# output should look like this:\n# \"{'rec.sport.baseball': 7, 'sci.space': 11, 'talk.religion.misc': 15, 'comp.sys.mac.hardware': 14, \n# 'misc.forsale': 13, 'alt.atheism': 0, 'sci.electronics': 6, 'talk.politics.misc': 18, \n# 'rec.sport.hockey': 8, 'rec.autos': 2, 'rec.motorcycles': 4, 'sci.crypt': 16, 'comp.windows.x': 12, \n# 'talk.politics.guns': 5, 'comp.graphics': 10, 'talk.politics.mideast': 1, 'soc.religion.christian': 19, \n# 'comp.sys.ibm.pc.hardware': 17, 'comp.os.ms-windows.misc': 3, 'sci.med': 9}\"\n\n# Load the data into RDDs\n# REDUCED  the first version below only loads the 4 'rec....' topics, use the second to get the full set. \n>>> %time newsGroup_RDD = sc.wholeTextFiles(p + ... ) # use a glob expression that catches only the directories starting with rec \n# FULL use all topics, it can take several minutes to read 20000 files on DSX\n#%time newsGroup_RDD = sc.wholeTextFiles(p + '/*') \n\n#print the total number of documents here (2000 for *comp.sys*, 20000 for the full set):\n#print ('Number of documents read is:',newsGroup_RDD.count())\n%time newsGroup_RDD.take(1)", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "### Comment \nYou can see a significant difference between the time for `wholeTextFiles` and `take`. This is because only an *action* actually triggers the execution. Try getting everything to work with the REDUCED version first, then switch to the FULL version.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# 2) Remove the headers,  prepare the topics and the labels\nAs in the main lab, we need to remove these headers and from the file paths, we keep only the topics (newsgroup names). \n\nIn addition we add lables, using the `topicLabels` dictionary. \n\nFinally, we save the processed RDD in pickle format (creates a directory of files). This throws and error if the directory exists, read the instructions below.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import re\n\n# new function to remove the headers using regular expressions\ndef removeHeader(ft): \n    fn,text = ft # unpack the filename and text content \n    matchObj = re.match(r'.+^Lines:(.*)', text,re.DOTALL|re.MULTILINE) \n    if(matchObj): # only if the pattern has matched \n        text = matchObj.group(1) # can we replace the text, \n    return (fn,text)\n\nft_RDD = newsGroup_RDD.map(removeHeader)\n\n# Remove the path before the last directory name and the file name after (i.e. leave the directory names, which are the newsgroup topics) \ntt_RDD = ft_RDD.map(lambda ft: (re.split('[/]',ft[0])[-2],ft[1]))\n\n# Now add the topic numbers as labels\n#>>> add the labels as a third component to each RDD element. \n#>>> This third element is determined by reading the label from the \n#>>> topicLabels dictionary, using the topic string (first in the RDD elements) as key.\n>>> %time ttl_RDD = tt_RDD.map(lambda ...)\n\n# and show an example result\nprint(ttl_RDD.take(1))\n# it should contain [(<topic name>, <text>, <labelNumber>)]\n\n# Reading and preparing the RDD can take some time, so saving it in a file for later use is a good idea. \n%time ttl_RDD.saveAsPickleFile('ttl_RDD.pkl')\n# WARNING: this method will THROW AN ERROR IF THE FILE/DIRECTORY EXISTS\n# It's a good idea tho change the name once you switch to FULL, so that you keep both versions\n# If you are happy with the data you have, you can just comment out the code above\n\n# To delete the directory use the following line\n# %rm -Rf ~/notebook/work/ttl_RDD.pkl\n# and \n# %ls -lh ~/notebook/work\n# to check that it's gone", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "## 3) DataFrames\n\nIn this section we will introduce Dataframes as before. The main difference is that we treat the labels differently. In the previous version we generated the labels with `df.withColumn()`, which created them as a numeric type automaticaly. Here we have created them already in the RDDs and need to set the schema so that a numeric column will be created. \n\nHere are links to the docs on StructField, DataType (needed to define the column types), and StructType:\n[http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.types.StructField](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.types.StructField)\n\n[http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.types.DataType](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.types.DataType)\n\n[http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.types.StructType](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.types.StructType)\n\nWe create three colums (topic, text, label), the first two as strings, the last as integer.\n\nLike the RDDs, we also save the DataFrames for further use. For DataFramew we use the Parquet format, which is an efficient format for storing large structured data. For more information see [http://parquet.apache.org](http://parquet.apache.org). ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.sql.types import *\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# The schema is represented as a StructField object that comprises three fields, name (a string), dataType (a DataType) and nullable (a bool). \n# We create 3 fields of names and types according to our data: two strings, one integer for thelabel. \nfields = [] \nfields.append(StructField('topic', StringType(), True))\n#>>> now do the same for 'text' instead of topic:\n>>> fields.append(StructField(...))\n#>>> and now for 'label' with type IntegerType instead of StringType:\n>>> fields.append(StructField(...))\n\n# the fields together define our schema\nschema = StructType(fields)\n\n# If you want to use a previously saved RDD, load it like this\nttl_RDD = sc.pickleFile('ttl_RDD.pkl')\n\n# Apply the schema in createDataFrame, to create a DataFrame 'df' from the RDD\ndf = sqlContext.createDataFrame(ttl_RDD, schema)\n\n# print the schema of our DataFrame\ndf.printSchema()\n# output should look like this:\n# root\n# |-- topic: string (nullable = true)\n# |-- text: string (nullable = true)\n# |-- label: integer (nullable = true)\n\n# Use pixiedust to show the data by topic\nimport pixiedust\ndisplay(df.select('topic'))# you can switch to bar or pie charts or other presentations ", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "pieChart", 
                        "mpld3": "false", 
                        "rendererId": "matplotlib", 
                        "keyFields": "topic"
                    }
                }
            }
        }, 
        {
            "source": "# 4) Create training and test set\n\nThe dataframe can be split into train and test set. For testing purposes, we can also downsample.\n\nThen we create a random split into training and test set. The test and tratining sets will be used multiple times, therefore caching is helpful to speed things up. A call to `DataFrame.cache()` is shorthand for `DataFrame.persist(MEMORY_AND_DISK)` used in the cell after the next. For information on persistence and StorageLevels see here: \n[http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence](http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence) \nand here:\n[http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.StorageLevel](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.StorageLevel)\nThese StorageLevels levels apply to RDDs and DataFrames. \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# REDUCED Downsample the data for testing\n#df2 = df.sample(False,0.1)\n# FULL keep all data when ready\ndf2 = df\n\n# It's useful to save this Dataframe, but once the file exists, ONCE THE FILE EXISTS, THE METHOD WILL THROW AN ERROR\ndf.write.parquet('df.pqt') \n# to avoid errors, comment this line out, use the additional argument 'overwrite' \n\n#Create the training and testing set from the dataframe above\n#randomSplit - splits the Df into training/testing using the weights \n#you can try other combinations of weights\ntrain_set, test_set = df2.randomSplit([0.8, 0.2])\n\n# cache() makes sure the sets are stored in memory or disk, rather than re-computed.\ntrain_set.cache()\ntest_set.cache()\n\ntrain_set.write.parquet('train_set.pqt')\ntest_set.write.parquet('test_set.pqt')\n\n#>>> print the counts of training ans test set and time the execution with the %time magic\n>>> %time print (\"Training-set count:\", ... )\n>>> %time print (\"Test-set count:\", ... )", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "# When the previous steps are done, you can save time by starting from here the next time.\n\nOnce the above has been executed, you can use the cell below to load the prepared data, which is much faster. \nYou will need to store new files (you can use different file names) to switch from the REDUCED to the FULL dataset. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "%%time \n# Time the execution for the whole cell (except the first line)\n%cd ~/notebook/work/\ndf = spark.read.parquet('df.pqt')\ntrain_set = spark.read.parquet('train_set.pqt')\ntest_set = spark.read.parquet('test_set.pqt')\nprint(\"+++ all DataFrames loaded +++\")\n\n# make sure that the test and training sets are cached, equvalent to cache()\ntrain_set.persist(StorageLevel.MEMORY_AND_DISK)\ntest_set.persist(StorageLevel.MEMORY_AND_DISK)", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "## 4) Using ML to classify messages \n\nWe now use an ML Pipeline, but with MultiClass classifier and evaluator. \n\nWe use DecisionTreeClassifier as it can deal with multiple classes out of the box and tends to work well on multiclass problems. We also use LogisticRegression, which comes with a multinomial implementation in Spark. Interestingly, it performs substantially worse in my experiments on this dataset. \nWe can also use any binary classifier that outputs probabilities with the OneVsAll class:\n[http://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.classification.OneVsRest](http://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.classification.OneVsRest)\n    \nTo read more on Spark ML:\n[https://spark.apache.org/docs/2.1.0/ml-features.html](https://spark.apache.org/docs/2.1.0/ml-features.html)\n \nNOTE: The Spark `RandomForestClassifier` gives the best results on a full dataset, but crashes on small datasets in CV/TV validation. Feel free to play around with it, but be aware. This occasionally also happens for `DecisionTreeClassifier`. This is a know error in Spark 2.1.0:[https://issues.apache.org/jira/browse/SPARK-18036](https://issues.apache.org/jira/browse/SPARK-18036).\nIf you encounter this error ( I'll contain this \"Py4JJavaError: \\[...\\] java.lang.UnsupportedOperationException: empty.maxBy\"), the best option is to use a larger dataset (more samples or more classes).\n\nAlternatively, try LogisticRegression, but it's doesn't perform well in this case.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.ml.linalg import Vector\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression, RandomForestClassifier\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import HashingTF,StopWordsRemover,IDF,Tokenizer\n\n# Construct a pipeline\n# We split each sentence into words using the Spark Tokenizer. \ntokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n\n#Remove stopwords with the Spark SW Remover\nremover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n\n#For each message (bag of words),use HashingTF to hash the sentence into a feature vector. \nhashingTF = HashingTF().setNumFeatures(1000).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n\n#We use IDF to rescale the feature vectors; this generally improves performance when using text as features.\nidf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)\n\n#Our feature vectors could then be passed to a learning algorithm.\n# REDUCED\ncf = DecisionTreeClassifier()\n# FULL\n# cf = LogisticRegression()\n# or\n#cf = RandomForestClassifier()\n# WARNING: The RandomForestClassifier tends to crash in the CV/TV validation with small numbers of classes, it works fine on the full dataset.\n\n#Then we connect all the steps above to create one pipeline of feature extraction, transformation and classification:\n>>> stgs=[...] # add the stages here\npipeline=Pipeline(stages=stgs) \nprint (\"Pipeline:\",pipeline.explainParams())\n# output should look similar to this: \"Pipeline: stages: a list of pipeline stages (current: \n# [Tokenizer_44b290cf2be2d9324136, StopWordsRemover_4845b8a0b79787adf72f, HashingTF_4a24b794e7b3fa4f5eeb, \n# IDF_40d89e759a5e90b76752, DecisionTreeClassifier_4b28b4c0e9dd1b11ea1f])\"", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "#We can get information for each parameter  using the .explainParams()\nprint (\"Tokenizer:\",tokenizer.explainParams())\nprint(\"\\n\\n\")\nprint (\"Remover:\",remover.explainParams())\nprint(\"\\n\\n\")\nprint (\"HashingTF:\",hashingTF.explainParams())\nprint (\"\\n\\n\")\nprint (\"IDF:\",idf.explainParams())\nprint (\"\\n\\n\")\nprint (\"classifier:\",cf.explainParams())", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "# After we construct this ML pipeline,we can fit it to the training data\n# and obtain a trained pipeline model that can be used for prediction.\n%time model=pipeline.fit(train_set)", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "## 5) Evaluate prediction results", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#After we obtain a fitted pipeline model, we want to know how well it performs. \n#Let us start with some manual checks by displaying the predicted labels.\n\n#You can simply use the .transform() on the test set to make predictions on the test set\ntest_predictions = model.transform(test_set)\n# this adds the prediction to every row in the table\n#>>> and do the same for the training set\n>>> train_predictions = model.transform(...)\n\n#Show the predicted labels along with true labels and raw texts.\n#The '%' in the search pattern is like a '*' in a glob pattern. \ntest_predictions.select(\"topic\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"%hockey%\")).show(5)\n#>>> and show some of another class, how about baseball?\n>>> test_predictions.select(\"topic\",\"prediction\",\"label\").filter(test_predictions.topic.like(...)).show(5)", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "The predicted labels give as an intuition of what is going on. Now, let's evaluate the model quantitatively with an MulticlassClassificationEvaluator: [http://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator](http://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator)\n\nFirst use accuracy, but feel free to try other metrics. See the available choices in MulticlassClassificationEvaluator here:\n[http://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator.metricName](http://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator.metricName)\n\nThe evaluator uses the label and prediction information in the tables to calculate metrics like F1 or accuracy. The defnitions and additional metrics available through `MulticlassMetrics` are given here: \n[https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#label-based-metrics](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#label-based-metrics)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#>>> we want to use accuracy now instead of areaUnderROC \n>>> evaluator = MulticlassClassificationEvaluator().setMetricName(...)\n\n\nprint (\"Accuracy - training:\",evaluator.evaluate(train_predictions))\nprint (\"Accuracy - testing:\",evaluator.evaluate(test_predictions))", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "The training and test results are usually good or OK on small tasks (few classes). They do go down to substantially for the full set of 20 classes (why is that normal?).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Tuning the Hyper-Parameters", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#We use a ParamGridBuilder to construct a grid of parameters to search over.\n# With 3 values for hashingTF.numFeatures and 3 values for idf,\n# this grid will have 3 x 3 = 9 parameter settings for the Validator to choose from.\n\nparamGrid = ParamGridBuilder()\\\n    .addGrid(hashingTF.numFeatures,[100,1000,10000])\\\n    .addGrid(idf.minDocFreq,[0,10,100])\\\n    .build()\n    \n# This grid can be extended with other parameters, e.g. those specific to the classifier ", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "# REDUCED A TrainValidationSplit uses only one split of the data, is therefore cheaper to run\nfrom pyspark.ml.tuning import TrainValidationSplit \nvalidator = TrainValidationSplit().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid)\n\n# FULL A cross validaor creatues mutliple splits, which can improve results \n#from pyspark.ml.tuning import CrossValidator\n#validator = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n\n# A validator requires an Estimator, a grid of Paramters, and an Evaluator.\n# Note: This can take a long time to run with the full data\n%time tunedModel = validator.fit(train_set)", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "# >>> now calculate the training and test accuracy for the tuned model, we need to do the transformations\n>>> print(\"Training accuracy for tuned model =\",evaluator.evaluate(tunedModel.transform(...))) \n>>> print(\"Test accuracy for tuned model =\",evaluator.evaluate(tunedModel.transform(...)))\nprint (\"Test accuracy for default model:\",evaluator.evaluate(test_predictions)) # re-used from above", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "Unfortunately, getting the tuned hyper-parameter values is not straighforward. It requires to get the `validationMetrics` from the tunedModel (the results for each parameter map in the grid) and finding the corresponding parameter map in the grid. \n\nI've written a function for you to use below.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def bestValidationParamters(vaidatedModel,parameterGrid):\n    \"\"\" Find the paramter map that produced the highest result in a validation (TrainValidationSplit or CrossValidation) \n        \n        Positional arguments:\n        validatedModel: the model returned by cv.fit() or tvs.fit()\n        parameterGrid: the paramterGrid used in the fitting\n    \"\"\"\n    # link the measured metric results to the paramter maps in the grid\n    metricParamPairs = zip(vaidatedModel.validationMetrics,parameterGrid)\n    # for our metrics, higher is better and 0 is the minimum\n    bestMetric = 0 # initialize with the minimal value\n    # now iterate through all tested parameter maps\n    for metric,params in metricParamPairs:\n        if metric > bestMetric: # if metric is better than current best\n            bestParams = params # then keept the corresponding parameter map \n    return bestParams # and return the final best paramters\n\nbestValidationParamters(tunedModel,paramGrid) # will return a parameter dictrionary {<parameter>: <value>, ...} ", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {}
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }
}