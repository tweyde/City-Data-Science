{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Lab Sheet 4-5: More Text Classification, Semi-structured text processing and DataFrames\n\nThese tasks are for working in the lab session and during the week. We'll build on last weeks code and add some classification. \n\nFor this lab, we are going to use a simple text classification problem as an example to build an ML pipeline in Spark MLlib, inspect it if it doesn't work as expected, and tune hyperparameters. We will also do some web scraping and \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n\n## PART 1 - LAB\n## 1) Dataset: Newsgroups\n\nNow we use anoter, larger dataset, which consists of 'usenet' discussions from the early days of the Internet. This dataset contains messages from 20 different newsgroups on different topics with ~1000 messages each. More information and the data can be found here here:\n\n[http://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/](http://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/)\n\nWith the larger dataset you should get more meaningful time measurements. Try several runs and try executing things in different oder (here we are only training a LR classifier, however you are encouraged to try other models). \nTo create a meaningful dataset for classification, you need read in at least 2 topics and then use `RDD.randomSplit()`. For this lab , we will use alt.atheism and comp.graphics. Try adding more topics to the dataset, there are 20 differet directories (i.e. topics).\n\n\nTo download and unpack the data, use the following code:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/datasets\n--2018-03-01 22:14:44--  http://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/20_newsgroups.tar.gz\nResolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\nConnecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 17332201 (17M) [application/x-gzip]\nSaving to: \u201820_newsgroups.tar.gz.1\u2019\n\n100%[======================================>] 17,332,201  3.07MB/s   in 7.7s   \n\n2018-03-01 22:14:53 (2.15 MB/s) - \u201820_newsgroups.tar.gz.1\u2019 saved [17332201/17332201]\n\n20_newsgroups\t      20_newsgroups.tar.gz.1  lingspam_public02.tar.gz\n20_newsgroups.tar.gz  lingspam_public\n"
                }
            ], 
            "source": "%cd ~/notebook/work/City-Data-Science/datasets/\n!wget http://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/20_newsgroups.tar.gz\n!ls"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": ">>> Unzipping finished.\n"
                }
            ], 
            "source": "!tar -xf 20_newsgroups.tar.gz\n# '!' calls a program on the machine (the DSX service runs on virtual Linux machines).\nprint(\">>> Unzipping finished.\")"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!echo \"datasets/20_newsgroups/**\" >> ~/notebook/work/City-Data-Science/.git/info/exclude\n# add the newly created directory to the list of excluded dirs to prevent accidental uploading to github\n# do this only once"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Errno 2] No such file or directory: '20_newsgroups/ # go into the new directory'\n/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/datasets\n\u001b[0m\u001b[01;34m20_newsgroups\u001b[0m/        20_newsgroups.tar.gz.1  lingspam_public02.tar.gz\n20_newsgroups.tar.gz  \u001b[01;34mlingspam_public\u001b[0m/\n"
                }
            ], 
            "source": "%cd 20_newsgroups/ # go into the new directory\n%ls # and show its content"
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Pixiedust database opened successfully\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/html": "\n        <div style=\"margin:10px\">\n            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n            </a>\n            <span>Pixiedust version upgraded from 1.1.7 to 1.1.7.1</span>\n        </div>\n        ", 
                        "text/plain": "<IPython.core.display.HTML object>"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771\np:  /gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/datasets/20_newsgroups\nNumber of documents read is: 2000\n"
                }
            ], 
            "source": "from pyspark.ml import *\nfrom pyspark.ml.classification import *\nfrom pyspark.ml.feature import *\nfrom pyspark.ml.param import *\nfrom pyspark.ml.tuning import *\nfrom pyspark.ml.evaluation import *\nfrom pyspark.sql import *\nimport pixiedust\nfrom pyspark.sql.types import Row\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\nspark = SparkSession.builder.getOrCreate()\n\nimport os.path\n%cd ~\np = os.path.abspath('./notebook/work/City-Data-Science/datasets/20_newsgroups/')\nprint(\"p: \", p) # we need the path, as the executors that will read the from directories, will not run in the same environment, so that %cd calls don't help\n\n#here we are setting the path to select 2 topics\ndirPath1 = p + '/alt.atheism'\ndirPath2 = p + '/comp.graphics'\n\n# Use wholeTextFiles to read both the files\n>>> alt_rdd = sc.\n>>> comp_rdd = sc.\n\nalt_rdd.take(1)\ncomp_rdd.take(1)\n\n#Create a union of the 2 RDD's so we hava a full set\n>>> newsGroup_RDD = ....\n\n#printing the total number of documents here:\nprint ('Number of documents read is:',newsGroup_RDD.count())"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[('alt.atheism', 'Path: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!zaphod.mps.ohio-state.edu!howland.reston.ans.net!bogus.sura.net!news-feed-1.peachnet.edu!umn.edu!uum1!mac.cc.macalstr.edu!acooper\\nFrom: acooper@mac.cc.macalstr.edu (Turin Turambar, ME Department of Utter Misery)\\nNewsgroups: alt.atheism\\nSubject: Re: free moral agency\\nMessage-ID: <1993Apr20.185237.4924@mac.cc.macalstr.edu>\\nDate: 20 Apr 93 18:52:37 -0600\\nReferences: <1quuaa$6s@eagle.lerc.nasa.gov> <735295730.25282@minster.york.ac.uk>\\nDistribution: world\\nOrganization: Macalester College\\nLines: 19\\n\\nIn article <735295730.25282@minster.york.ac.uk>, cjhs@minster.york.ac.uk writes:\\n> : Are you saying that their was a physical Adam and Eve, and that all\\n> : humans are direct decendents of only these two human beings.?  Then who\\n> : were Cain and Able\\'s wives?  Couldn\\'t be their sisters, because A&E\\n> : didn\\'t have daughters.  Were they non-humans?\\n> \\n> Genesis 5:4\\n> \\n> and the days of Adam after he begat Seth were eight hundred years, and\\n> he begat sons and daughters:\\n> \\n> Felicitations -- Chris Ho-Stuart\\n\\n\\nIt is still incestuous.... :)\\n\\n\\n\\n--Adam \"What happened to my sig?\"  Cooper\\n'), ('alt.atheism', \"Newsgroups: alt.atheism\\nPath: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!apple!mumbo.apple.com!gallant.apple.com!sandvik-kent.apple.com!user\\nFrom: sandvik@newton.apple.com (Kent Sandvik)\\nSubject: Re: some thoughts.\\nSender: news@gallant.apple.com\\nMessage-ID: <sandvik-150493145812@sandvik-kent.apple.com>\\nDate: Thu, 15 Apr 1993 22:00:41 GMT\\nReferences: <bissda.4.734849678@saturn.wwc.edu>\\nOrganization: Cookamunga Tourist Bureau\\nFollowup-To: alt.atheism\\nLines: 24\\n\\nIn article <bissda.4.734849678@saturn.wwc.edu>, bissda@saturn.wwc.edu (DAN\\nLAWRENCE BISSELL) wrote:\\n> \\n> \\tFirst I want to start right out and say that I'm a Christian.  It \\n> makes sense to be one.  Have any of you read Tony Campollo's book- liar, \\n> lunatic, or the real thing?  (I might be a little off on the title, but he \\n> writes the book.  Anyway he was part of an effort to destroy Christianity, \\n> in the process he became a Christian himself.\\n\\nSeems he didn't understand anything about realities, liar, lunatic\\nor the real thing is a very narrow view of the possibilities of Jesus\\nmessage.\\n\\nSigh, it seems religion makes your mind/brain filter out anything\\nthat does not fit into your personal scheme. \\n\\nSo anyone that thinks the possibilities with Jesus is bound to the\\nclassical Lewis notion of 'liar, lunatic or saint' is indeed bound\\nto become a Christian.\\n\\nCheers,\\nKent\\n---\\nsandvik@newton.apple.com. ALink: KSAND -- Private activities on the net.\\n\")]\n"
                }
            ], 
            "source": "#Split the filename and content\n#You can use the splitFileWords function from Lab 3\n#This function will help split the filename and content\n\nimport re \n    \ndef splitFileWords(filenameContent): # your splitting function\n    f,c = filenameContent # split the input tuple  \n    fwLst = [] # the new list for (filename,word) tuples\n    wLst = re.split('\\W+',c) # <<< now create a word list wLst\n    for w in wLst : # iterate through the list\n        fwLst.append((f,w)) # and append (f,w) to the \n    return fwLst #return a list of (f,w) tuples\n\n\n# Remove the file name and path before the last directory name (i.e. the newsgroup name) \nfnt_RDD = newsGroup_RDD.map(lambda ft: (re.split('[/]',ft[0])[-2],ft[1]))\nprint(fnt_RDD.take(2)) "
        }, 
        {
            "source": "## 2) Preprocessing: Remove the headers from the files\n\nAt closer inspection, we can see that the messages have headers, and one of them starts with 'Newsgroups:' and actually lists the topic. This is clearly an unreasonable shortcut for the classifier, as we are interested in predicting topics from the text.  \n\nThus, the dataset needs preprocessing to remove these headers. We can use a regular expression to remove the header. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 90, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 90, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('alt.atheism',\n  ' 19\\n\\nIn article <735295730.25282@minster.york.ac.uk>, cjhs@minster.york.ac.uk writes:\\n> : Are you saying that their was a physical Adam and Eve, and that all\\n> : humans are direct decendents of only these two human beings.?  Then who\\n> : were Cain and Able\\'s wives?  Couldn\\'t be their sisters, because A&E\\n> : didn\\'t have daughters.  Were they non-humans?\\n> \\n> Genesis 5:4\\n> \\n> and the days of Adam after he begat Seth were eight hundred years, and\\n> he begat sons and daughters:\\n> \\n> Felicitations -- Chris Ho-Stuart\\n\\n\\nIt is still incestuous.... :)\\n\\n\\n\\n--Adam \"What happened to my sig?\"  Cooper\\n'),\n ('alt.atheism',\n  \" 24\\n\\nIn article <bissda.4.734849678@saturn.wwc.edu>, bissda@saturn.wwc.edu (DAN\\nLAWRENCE BISSELL) wrote:\\n> \\n> \\tFirst I want to start right out and say that I'm a Christian.  It \\n> makes sense to be one.  Have any of you read Tony Campollo's book- liar, \\n> lunatic, or the real thing?  (I might be a little off on the title, but he \\n> writes the book.  Anyway he was part of an effort to destroy Christianity, \\n> in the process he became a Christian himself.\\n\\nSeems he didn't understand anything about realities, liar, lunatic\\nor the real thing is a very narrow view of the possibilities of Jesus\\nmessage.\\n\\nSigh, it seems religion makes your mind/brain filter out anything\\nthat does not fit into your personal scheme. \\n\\nSo anyone that thinks the possibilities with Jesus is bound to the\\nclassical Lewis notion of 'liar, lunatic or saint' is indeed bound\\nto become a Christian.\\n\\nCheers,\\nKent\\n---\\nsandvik@newton.apple.com. ALink: KSAND -- Private activities on the net.\\n\"),\n ('alt.atheism',\n  ' 17\\n\\n>DATE:   20 Apr 93 05:23:15 GMT\\n>FROM:   Bake Timmons <timmbake@mcl.ucsb.edu>\\n>\\n>>Remember, Koresh \"dried\" for your sins.\\n>>\\n>>And pass that beef jerky.  Umm Umm.\\n>\\n>Though I wasn\\'t there, at least I can rely on you now to keep me posted on what\\n>what he\\'s doing.\\n>\\n\\nWhat\\nA \\nCook\\nOff !\\n\\n\\n')]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import re\n\n# new function to remove the headers using regular expressions\ndef removeHeader(ft): \n    fn,text = ft # unpack the filename and text content \n    # now use a regular expression to match the text\n    # When you check the data, you can see that the first line that \n    # starts with 'Lines:' normally ends the headers. \n    # Only the very first file is different, but we can tolerate one wrong sample for now. \n    # (How could we be more thorough?)\n#>>> matchObj = re.match()  ..  #fill in the expression here   \n>>>    matchObj = re.match(r'.+^Lines:(.*)', .........) \n    if(matchObj): # only if the pattern has matched \n        text = matchObj.group(1) # can we replace the text, \n        #otherwise we keep the old for now (what could be a better solution?)\n    return (fn,text)\n\nfnt_RDD2 = fnt_RDD.map(removeHeader)\nfnt_RDD2.take(3)"
        }, 
        {
            "source": "## 3) DataFrames\n\nIn this section we will introduce Dataframes. To read more on DataFrames look here:\nhttps://spark.apache.org/docs/latest/sql-programming-guide.html\n\nIn Spark we can create Datframes from RDDs and that is what we will implement in the next section. \nA dataframe represents a table structure. We defined a schema that contains the names and types of the coumns in the table.\n\nFrom the official documentation:\n\nA Datframe can be created programatically in 3 steps:\n\n- Create an RDD of Rows from the original RDD;\n- Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.\n- Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.\n\nMore on pyspark API can be found here:\n\nhttp://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "barChart", 
                        "keyFields": "topic"
                    }
                }
            }, 
            "outputs": [], 
            "source": "from pyspark.sql.types import *\nfrom pyspark.sql import Row\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# The schema is encoded in a string.\n#Here we are only interested in the topic and text\nschemaString = \"topic text\"\n\n# A StructField object comprises three fields, name (a string), dataType (a DataType) and nullable (a bool). \n# We create 2 fields of strings with names according to our schemaString\nfields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n# these together define our schema\nschema = StructType(fields)\n\nprint(fnt_RDD.take(1))\n# Apply the schema in createDataFrame, to create a DataFrame 'df' from the RDD\ndf = sqlContext.createDataFrame(fnt_RDD, schema)\n\n#print the schema of our DataFrame\ndf.printSchema()\n\n#Use pixiedust to show the number of topics by frequency\n#there are only 2 topics here, so lets see them\ndisplay(df.select('topic'))"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------------+\n|        topic|\n+-------------+\n|comp.graphics|\n|  alt.atheism|\n+-------------+\n\n"
                }
            ], 
            "source": "# Create a (temporary) view using the DataFrame, so that we can us use SparkSQL.\ndf.createOrReplaceTempView(\"newsgroups\")\n\n# SQL can now be run on the DataFrame. \n# Let's start by selecting only the topics elements of each row \nresults = sqlContext.sql(\"SELECT DISTINCT topic FROM newsgroups\")\nresults.show()"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------------+----+\n|        topic| cnt|\n+-------------+----+\n|  alt.atheism|1000|\n|comp.graphics|1000|\n+-------------+----+\n\n"
                }
            ], 
            "source": "# We can make more sophisticated queries in SQL, e.g. using  topic names as a distinct feature and simply count number of files\nresults_topic = sqlContext.sql(\"SELECT DISTINCT topic, count(*) as cnt FROM newsgroups GROUP BY topic ORDER BY cnt DESC\")\nresults_topic.show()"
        }, 
        {
            "source": "We need numeric labels for the classifier, for now we go for binary labels. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----------+--------------------+-----+\n|      topic|                text|label|\n+-----------+--------------------+-----+\n|alt.atheism|Path: cantaloupe....|  0.0|\n|alt.atheism|Newsgroups: alt.a...|  0.0|\n|alt.atheism|Newsgroups: alt.a...|  0.0|\n+-----------+--------------------+-----+\nonly showing top 3 rows\n\n+-------------+--------------------+-----+\n|        topic|                text|label|\n+-------------+--------------------+-----+\n|comp.graphics|Newsgroups: comp....|  1.0|\n|comp.graphics|Xref: cantaloupe....|  1.0|\n|comp.graphics|Newsgroups: comp....|  1.0|\n+-------------+--------------------+-----+\nonly showing top 3 rows\n\n"
                }
            ], 
            "source": "# df.withColumn Returns a new DataFrame by adding a column with a name and value for each row.\n# The value is a 'column expression', where we compares with the string 'comp', to find out whether the topic is about computing.\n# double - will convert the resulting Boolean value into a number\nnews_Groups = df.withColumn(\"label\",df.topic.like(\"comp%\").cast(\"double\"))\n\n# you can use a syntax similar to an array to select some examples of either class \nalt_topic_df = news_Groups[df.topic.like(\"alt%\")]\nalt_topic_df.show(3)\n>>> Do the same for comp% \n....\ncomp_topic_df.show(3)"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Total document count: 2000\nTraining-set count: 1191\nTest-set count: 809\n"
                }
            ], 
            "source": "#Create the training and testing set from the dataframe above\n#randomSplit - splits the Df into training/testing using the weights \n#you can try other combinations of weights\ntrain_set, test_set = news_Groups.randomSplit([0.6, 0.4], 123)\nprint (\"Total document count:\",news_Groups.count())\nprint (\"Training-set count:\",train_set.count())\nprint (\"Test-set count:\",test_set.count())"
        }, 
        {
            "source": "## 4) Using ML to classify messages \n\nML is the Spark machine learning library for DataFrames. We want to build an ML pipeline to predict the Binary label.\n\nA Spark ML Pipeline is a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage.\n\nA practical ML pipeline might consist of many stages like feature extraction, feature transformation, and model fitting. We use  pipeline that consists of the following stages:\n\n    a)RegexTokenizer - which tokenizes each article into a sequence of words with a regex pattern,\n    b)HashingTF, which maps the word sequences produced by RegexTokenizer to sparse feature vectors using the hashing trick,\n    c)LogisticRegression, which fits the feature vectors and the labels from the training data to a logistic regression model.\n    \nTo read more on this:\n\nhttps://spark.apache.org/docs/2.1.0/ml-features.html\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 51, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.ml.linalg import Vector\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.feature import HashingTF,StopWordsRemover,IDF,Tokenizer\n\n#Constructing a pipeline\n#We split each sentence into words using Tokenizer. \n#Tokenizer only splits by white spaces\ntokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n\n#Remove stopwords\nremover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n\n#For each sentence (bag of words),use HashingTF to hash the sentence into a feature vector. \nhashingTF = HashingTF().setNumFeatures(1000).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n\n#We use IDF to rescale the feature vectors; this generally improves performance when using text as features.\nidf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)\n\n#Our feature vectors could then be passed to a learning algorithm.\n# Create a Logistic regression model\n>>> lr = .....\n#nb = NaiveBayes()\n\n#Then basically we connect all the steps above to create one pipeline\npipeline=Pipeline(stages=[tokenizer,remover,hashingTF,idf, lr])"
        }, 
        {
            "execution_count": 92, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Tokenizer: inputCol: input column name. (current: text)\noutputCol: output column name. (default: Tokenizer_42f0be3eb4c8e2df2abc__output, current: words)\n/n/n\nRemover: caseSensitive: whether to do a case sensitive comparison over the stop words (default: False, current: False)\ninputCol: input column name. (current: words)\noutputCol: output column name. (default: StopWordsRemover_4874a9e0896073939620__output, current: filtered)\nstopWords: The words to be filtered out (default: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'])\n/n/n\nHashingTF: binary: If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default False. (default: False)\ninputCol: input column name. (current: filtered)\nnumFeatures: number of features. (default: 262144, current: 1000)\noutputCol: output column name. (default: HashingTF_4a85a7f62489113aae61__output, current: rawFeatures)\n/n/n\nIDF: inputCol: input column name. (current: rawFeatures)\nminDocFreq: minimum number of documents in which a term should appear for filtering (default: 0, current: 0)\noutputCol: output column name. (default: IDF_4d8c8a71263025118e48__output, current: features)\n/n/n\nPipeline: stages: a list of pipeline stages (current: [Tokenizer_44edb325f565f0ae234a, StopWordsRemover_4bc2ab7107aead1e332b, HashingTF_40f9b80b6ea507d75f99, IDF_4d73a5eaecb14fdd4664, LogisticRegression_494cb5d9cb5513547140])\n/n/n\n"
                }
            ], 
            "source": "#We can get an information for each parameter  using the .explainParams()\nprint (\"Tokenizer:\",tokenizer.explainParams())\nprint(\"/n/n\")\n\n#Use the same function as above to get details for Remover, Hashing TF , IDF and Pipeline\n>>>print (\"Remover:\",......)\nprint(\"/n/n\")\n>>>print (\"HashingTF:\",......)\nprint (\"/n/n\")\n>>>print (\"IDF:\",.......)\nprint(\"/n/n\")\n\n>>>print (\"Pipeline:\",.......)\nprint(\"/n/n\")\n"
        }, 
        {
            "execution_count": 93, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "CPU times: user 7 \u00b5s, sys: 3 \u00b5s, total: 10 \u00b5s\nWall time: 19.3 \u00b5s\n"
                }
            ], 
            "source": "#Use the pipeline option to fit the training set and create a model\n\n# Jupyter offers a simpler way to take the time thanwe used in the coursework. \n%time\n\n# After we construct this ML pipeline,we can fit it to the training data\n# and obtain a trained pipeline model that can be used for prediction.\nmodel=pipeline.fit(train_set)"
        }, 
        {
            "source": "## 4) Evaluate prediction results", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 94, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----------+--------------------+----------+-----+\n|      topic|         probability|prediction|label|\n+-----------+--------------------+----------+-----+\n|alt.atheism|[0.99999999999987...|       0.0|  0.0|\n|alt.atheism|[0.99999933900794...|       0.0|  0.0|\n|alt.atheism|[0.08125659188165...|       1.0|  0.0|\n|alt.atheism|           [1.0,0.0]|       0.0|  0.0|\n|alt.atheism|[4.11055589470913...|       1.0|  0.0|\n+-----------+--------------------+----------+-----+\nonly showing top 5 rows\n\n+-------------+--------------------+----------+-----+\n|        topic|         probability|prediction|label|\n+-------------+--------------------+----------+-----+\n|comp.graphics|[9.23259183148201...|       1.0|  1.0|\n|comp.graphics|[7.01056035665729...|       1.0|  1.0|\n|comp.graphics|[3.41028828737437...|       1.0|  1.0|\n|comp.graphics|[1.63245286744783...|       1.0|  1.0|\n|comp.graphics|[1.56693132031322...|       1.0|  1.0|\n+-------------+--------------------+----------+-----+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "#After we obtain a fitted pipeline model, we want to know how well it performs. \n#Let us start with some manual checks by displaying the predicted labels.\n\n#You can simply use the .transform() on the test set to make predictions on the test set\ntest_predictions = model.transform(test_set)\ntrain_predictions = model.transform(train_set)\n\n#Show the predicted labels along with true labels and raw texts.\ntest_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").show(5)\n# and show some of the other class ...in our case for comp%\n>>> test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").filter(..............).show(5)\n"
        }, 
        {
            "execution_count": 95, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Area under ROC curve - training: 1.0\nArea under ROC curve - testing: 0.9627293507890521\n"
                }
            ], 
            "source": "#The predicted labels look accurate. \n#Let's evaluate the model quantitatively.\n\nevaluator = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\nprint (\"Area under ROC curve - training:\",evaluator.evaluate(train_predictions))\nprint (\"Area under ROC curve - testing:\",evaluator.evaluate(test_predictions))"
        }, 
        {
            "source": "The training result is already perfect, the test result is also excellent. So, this task is easy for LogisticRegression. With 20 classes, the task gets harder however. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Extra Tasks\n\nThese tasks are a bit more involved in terms of programming and ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Multi- class classification\n\nUse all the 20 toics in the dataset as class labels. The reading of the data is straightforward. You will need to use a different mapping from newsgroup names to class labels, though. Then you will need a different evaluation, as the ROC AUC is only defined for the binary case. \n\nThe perfomance will be lower, so that it is worth to try and tune the hyper-parameters. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Tuning the Hyper-Parameters", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 96, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#We use a ParamGridBuilder to construct a grid of parameters to search over.\n\n#With 3 values for hashingTF.numFeatures and 3 values for idf,\n# this grid will have 3 x 3 = 9 parameter settings for CrossValidator to choose from.\n\nparamGrid = ParamGridBuilder()\\\n    .addGrid(hashingTF.numFeatures,[1000,10000,100000])\\\n    .addGrid(idf.minDocFreq,[0,10,100])\\\n    .build()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n\ncv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(2)\n# Note: This takes a long time to run\n# Do this step only when you've done everything else first!\n%%time\ncvModel = cv.fit(train_set)\nprint(\"Area under the ROC curve for best fitted model =\",evaluator.evaluate(cvModel.transform(test_set)))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Task:\n# After you have run the cell above , print both results (with and without cross validation)\n#Observe the results\n>>> Print results for both \nprint (\"Area under ROC curve for non-tuned model:\",........)  <<< fill in here\nprint (\"Area under ROC curve for fitted model:\",.............) <<< fill in here"
        }, 
        {
            "source": "## PART 2 \n\n\n\n## 7)Analysing XML Data\n\nYou can complete the first section and come to this later on.\n\n\nThese tasks are for working in the lab session and during the week. Unlike previous labs, these tasks do not build on previous labs directly, but use the skills you have learned so far on a new XML dataset. \n\nTo get different datasets, use the link below and once you get the code working for one dataset, you can then try and experiment with the remaing sets.\n\nhttp://ratings.food.gov.uk/open-data/\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 7a)   Using the XML parser\nWe start by reading the dataset into and parsing it using the ElementTree parser. \nThe XML looks like this: \n\n`<FHRSEstablishment>\n    <Header>\n    <ExtractDate>2015-11-06</ExtractDate>\n    <ItemCount>1256</ItemCount>\n    <ReturnCode>Success</ReturnCode>\n    </Header>\n    <EstablishmentCollection>\n        <EstablishmentDetail>\n        <FHRSID>507136</FHRSID>\n        <LocalAuthorityBusinessID>PI/000081182</LocalAuthorityBusinessID>\n        <BusinessName>196</BusinessName>\n        <BusinessType>Restaurant/Cafe/Canteen</BusinessType>\n        <BusinessTypeID>1</BusinessTypeID>\n        <AddressLine1>Cambridge</AddressLine1>\n        <AddressLine2>Cambridgeshire</AddressLine2>\n        <PostCode>CB1 3NF</PostCode>\n        <RatingValue>5</RatingValue>\n        <RatingKey>fhrs_5_en-GB</RatingKey>\n        <RatingDate>2015-01-22</RatingDate>\n        <LocalAuthorityCode>027</LocalAuthorityCode>\n        <LocalAuthorityName>Cambridge City</LocalAuthorityName>\n            <LocalAuthorityWebSite>http://www.cambridge.gov.uk</LocalAuthorityWebSite>\n            <LocalAuthorityEmailAddress>env.health@cambridge.gov.uk</LocalAuthorityEmailAddress>\n        <Scores>\n            <Hygiene>5</Hygiene>\n            <Structural>0</Structural>\n            <ConfidenceInManagement>5</ConfidenceInManagement>\n        </Scores>\n        <SchemeType>FHRS</SchemeType>\n        <NewRatingPending>False</NewRatingPending>\n        <Geocode>\n            <Longitude>0.14503300000000</Longitude>\n            <Latitude>52.19734500000000</Latitude>\n        </Geocode>\n    </EstablishmentDetail>`\n    ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 8) Get the XML Data\n\n\nThe predefined `parseXML` function creates an XML parse tree for you. Start creating an RDD that contains just the parse trees instead of text. Then filter out the files that could not be parsed.\n \nWe provide the code for using ElementTree, but it's worth having a look for more info here: [https://docs.python.org/3.5/library/xml.etree.elementtree.html](https://docs.python.org/3.5/library/xml.etree.elementtree.html).\n\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 60, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work\nfatal: destination path 'City-Data-Science' already exists and is not an empty directory.\r\n"
                }
            ], 
            "source": "#Get the data\n\n%cd ~/notebook/work/City-Data-Science/\n!git clone https://github.com/tweyde/City-Data-Science.git\n"
        }, 
        {
            "execution_count": 62, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "remote: Counting objects: 6, done.\u001b[K\nremote: Total 6 (delta 2), reused 2 (delta 2), pack-reused 4\u001b[K\nUnpacking objects: 100% (6/6), done.\nFrom https://github.com/tweyde/City-Data-Science\n   ddd2162..5593edd  master     -> origin/master\nUpdating ddd2162..5593edd\nFast-forward\n datasets/foodhygiene.zip | Bin 0 -> 31777575 bytes\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 datasets/foodhygiene.zip\n/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/datasets\n\u001b[0m\u001b[01;34m20_newsgroups\u001b[0m/        20_newsgroups.tar.gz.1  \u001b[01;34mlingspam_public\u001b[0m/\n20_newsgroups.tar.gz  foodhygiene.zip         lingspam_public02.tar.gz\n"
                }
            ], 
            "source": "!git pull\n%cd datasets/\n%ls"
        }, 
        {
            "execution_count": 69, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "\u001b[0m\u001b[01;34m20_newsgroups\u001b[0m/          \u001b[01;34mfoodhygiene\u001b[0m/      lingspam_public02.tar.gz\r\n20_newsgroups.tar.gz    foodhygiene.zip\r\n20_newsgroups.tar.gz.1  \u001b[01;34mlingspam_public\u001b[0m/\r\n"
                }
            ], 
            "source": "!rm -R foodhygiene\n!unzip -q foodhygiene.zip\n%ls\n!echo \"datasets/foodhygiene/**\" >> ~/notebook/work/City-Data-Science/.git/info/exclude\n# add the newly created directory to the list of excluded dirs to prevent accidental uploading to github\n# do this only once"
        }, 
        {
            "execution_count": 71, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771\n[('file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/work/City-Data-Science/datasets/foodhygiene/FHRS524en-GB.xml', <Element 'FHRSEstablishment' at 0x7f52b5fd2f48>)]\n"
                }
            ], 
            "source": "import xml.etree.ElementTree as ET\n\ndef parseXML(f_x):\n    try:\n        root = ET.fromstring(f_x[1])\n    except ET.ParseError as err:\n        # parsing error :-(\n        root = None\n    return (f_x[0], root)\n\n%cd ~\np = os.path.abspath('./notebook/work/City-Data-Science/datasets/foodhygiene')\nrawData = sc.wholeTextFiles(p)\nparsedData = rawData.map(lambda ........) #<<<< map to XML parse trees\nparsedData = parsedData.filter(lambda .......) #<<< filter out items where the parse tree is `None`\nprint(parsedData.take(1)) "
        }, 
        {
            "source": "## 9) Find establishements with valid hygiene ratings\n\nFind all the possible RatingValue elements. For that we use the ElementTree function `findall`. We use a syntax called XPath, which enables us to find elements lower down in the tree without explicitly traversing it. The XPath syntax ist shown here: [https://docs.python.org/3.5/library/xml.etree.elementtree.html?highlight=elementtree#supported-xpath-syntax](https://docs.python.org/3.5/library/xml.etree.elementtree.html?highlight=elementtree#supported-xpath-syntax)\n\nFor finding the rating values, use `element.findall('.//RatingValue')` where element shoudl be the root of a parse tree. This gives a list of XML elements `x`, where we are interested in their `x.text` property.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 72, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 72, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "['4',\n '1',\n 'Pass',\n 'Pass and Eat Safe',\n 'Awaiting Inspection',\n '0',\n 'AwaitingPublication',\n 'Improvement Required',\n 'AwaitingInspection',\n 'Exempt',\n '5',\n '3',\n '2']"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "allRatings = parsedData.flatMap(lambda f_et: [x.text for x in f_et[1].findall('.//RatingValue')]).distinct() #<<< find distnct values\nallRatings.collect()"
        }, 
        {
            "source": "## 10) Remove non-numeric ratings\n\nNow we want to get rid of the non-numeric ratings. These should be:\n\n    invalidRatings = ['Pass','Pass and Eat Safe','Awaiting Inspection','AwaitingPublication','Improvement Required','AwaitingInspection','Exempt']\n    \n    \nFirst collect all XML elements tagged EstablishmentDetail (with finall as before), but keep the elements for further use (i.e. don't extract the text element as above).\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 73, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "497835\n411554\n"
                }
            ], 
            "source": "invalidRatings = ['Pass','Pass and Eat Safe','Awaiting Inspection','AwaitingPublication','Improvement Required','AwaitingInspection','Exempt']\n\nallEstData = parsedData.flatMap(lambda f_et: f_et[1].findall('.//EstablishmentDetail')) #<<< get the establishment detail\nprint(allEstData.count()) # should be 497835\n\nestData = allEstData.filter(lambda est: est.find('RatingValue').text not in invalidRatings)\nprint(estData.count()) # should be 411554"
        }, 
        {
            "source": "## 11) Find highest and lowest values\n\nFind the 10 local authorities with the highest and lowest mean hygiene rating. We now use find as there is only one local authority per establishment, which we use as key. We then get the RatingValue and average (not in reduce, so associativity is not required here).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from operator import add\n\nimport numpy as np\nauthRatings = estData.map(lambda est: \n    (est.find('LocalAuthorityName').text, \n    [float(est.find('RatingValue').text)])).reduceByKey(add).map(lambda a_rl: (a_rl[0], np.mean(a_rl[1])))\n\nauthRatings.... # using RDD.sortBy() get the 10 highest \nauthRatings.... # using RDD.sortBy() get the 10 lowest \n\n"
        }, 
        {
            "source": "## 12) Organise by PostCode\nUse the first part of the PostCode node (i.e. for IP7 5BY, only use IP7) to find the best and worse postcodes for food hygiene.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 86, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[('WF2 7EQ', 5.0), ('HD1 6EE', 5.0), ('DH1 5TU', 5.0), ('EX5 4AS', 5.0), ('LL55 3LP', 5.0), ('SO14 0DA', 5.0), ('WA14 1RU', 5.0), ('BT56 8EW', 5.0), ('TN32 5XF', 5.0), ('WC1X 8LR', 5.0)]\n[('W1U 4AP', 0.0), ('BB5 4JT', 0.0), ('WF13 4DJ', 0.0), ('SK1 3EH', 0.0), ('BL9 7AY', 0.0), ('CH41 2UW', 0.0), ('SN2 1QR', 0.0), ('DT10 2BS', 0.0), ('SN5 5PD', 0.0), ('HU5 5JR', 0.0)]\n"
                }
            ], 
            "source": "authRatings = estData.map(lambda est: \n    # organize by PostCode instead of LocalAuthority\n    (0 if est.find('PostCode') is None else est.find('PostCode').text, \n    [float(est.find('RatingValue').text)])).reduceByKey(add).map(lambda a_rl: (a_rl[0], np.mean(a_rl[1])))\n# output as above\n>>>\n>>>"
        }, 
        {
            "source": "## 13) Organise by Type\n\nUse the BusinessType or BusinessTypeID nodes to discover and consolidate all business types. Find the 10 best and worse rated business types for the entire UK, per local authority and per postcode using your consolidated categories", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 89, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[('School/college/university', 4.808328845808652), ('Hospitals/Childcare/Caring Premises', 4.7188041594454075), ('Farmers/growers', 4.609657947686117), ('Other catering premises', 4.6071905775226405), ('Retailers - supermarkets/hypermarkets', 4.583096085409252), ('Hotel/bed & breakfast/guest house', 4.530760695351991), ('Mobile caterer', 4.482452707110242), ('Manufacturers/packers', 4.470830070477682), ('Distributors/Transporters', 4.455373406193078), ('Importers/Exporters', 4.311475409836065)]\n[('Takeaway/sandwich shop', 3.7914874651810586), ('Retailers - other', 4.155244145840947), ('Pub/bar/nightclub', 4.219805857601133), ('Restaurant/Cafe/Canteen', 4.247219035792088), ('Importers/Exporters', 4.311475409836065), ('Distributors/Transporters', 4.455373406193078), ('Manufacturers/packers', 4.470830070477682), ('Mobile caterer', 4.482452707110242), ('Hotel/bed & breakfast/guest house', 4.530760695351991), ('Retailers - supermarkets/hypermarkets', 4.583096085409252)]\n"
                }
            ], 
            "source": "authRatings = estData.map(lambda est: \n    # organize by BusinessType instead of LocalAuthority\n    (0 if est.find('BusinessType') is None else est.find('BusinessType').text, \n    [float(est.find('RatingValue').text)])).reduceByKey(add).map(lambda a_rl: (a_rl[0], np.mean(a_rl[1])))\n# ouput as above\n>>>\n>>>"
        }, 
        {
            "source": "## 14) Web Scraping with an HTML parser\n\nThis is the scraper that was used to get the food hygiene data. This uses some programming techniques, we haven't covered. However, if you have some time, this is an interesting topic to look into, as the web is an boundless source of data. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#This is a code to scrape data from the web page. You can use this for future use \n#You do not have to run this today\n#This will take a *long* time\n\nimport urllib.request\nfrom html.parser import HTMLParser\n\nclass MyHTMLParser(HTMLParser): # created a \n    links = None\n    def handle_starttag(self, tag, attrs):\n        if self.links is None:\n            self.links = []\n        if tag == 'a':\n            href = None\n            for k,v in attrs: # keys and values of the tag attributes\n                if k == 'href': # if the key is 'href'\n                    href = v # we are interested in its value\n            if href is not None: # if there is a link\n                if href.endswith('en-GB.xml'): # and it is the type that we expect\n                    self.links.append(href) # then add to our list of links\n\nf = urllib.request.urlopen(\"http://ratings.food.gov.uk/open-data/en-GB\") # the ratings site\n\nparser = MyHTMLParser() # initiate our parser\nparser.feed(str(f.read())) # read from the URL\nfor l in parser.links: # get the collected links\n    fname = l.split('/') # split them\n    fname = fname[-1]    # get filename\n    print('downloading %s' % fname) # print a message\n    urllib.request.urlretrieve(l, fname) # and get the file\n"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}